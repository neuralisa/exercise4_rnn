{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 5-Fold Cross Validation Tutorial with PyTorch Example\n",
                "# 5-Fold Cross Validation Tutorial with PyTorch Example\n",
                "\n",
                "## What is Dropout?\n",
                "\n",
                "Dropout is a regularization technique used in neural networks to prevent overfitting. It was introduced in a research paper by Hinton et al. in 2012. The idea is to randomly drop out (i.e., set to zero) a fraction of the neurons during training, which helps to prevent the model from relying too heavily on any single neuron.\n",
                "\n",
                "## How Does Dropout Work?\n",
                "\n",
                "During training, each neuron has a probability `p` of being dropped out. This means that the neuron's output is set to zero with probability `p`, and the neuron is not updated during backpropagation. The remaining neurons have their outputs scaled up by a factor of `1/(1-p)` to maintain the same expected value.\n",
                "\n",
                "## Why Does Dropout Work?\n",
                "\n",
                "Dropout has several benefits:\n",
                "\n",
                "1. **Prevents overfitting**: By randomly dropping out neurons, the model is forced to learn multiple representations of the data, which helps to prevent overfitting.\n",
                "2. **Improves generalization**: Dropout helps the model to generalize better to new, unseen data.\n",
                "3. **Reduces co-adaptation**: Dropout breaks the co-adaptation between neurons, which helps to reduce overfitting.\n",
                "\n",
                "## What is 5-Fold Cross Validation?\n",
                "\n",
                "5-Fold Cross Validation is a technique used to evaluate the performance of a machine learning model. It involves splitting the dataset into 5 folds, training the model on 4 folds, and evaluating its performance on the remaining fold. This process is repeated 5 times, with each fold serving as the validation set once.\n",
                "\n",
                "## Why Do We Need 5-Fold Cross Validation?\n",
                "\n",
                "5-Fold Cross Validation helps to prevent overfitting and underfitting by providing a more accurate estimate of the model's performance. It also helps to reduce the variance of the model's performance by averaging the results over multiple folds.\n",
                "\n",
                "## Comparison with Dropout\n",
                "\n",
                "5-Fold Cross Validation and Dropout are both techniques used to prevent overfitting in machine learning models. However, they serve different purposes:\n",
                "\n",
                "* Dropout is a regularization technique that randomly drops out neurons during training to prevent overfitting.\n",
                "* 5-Fold Cross Validation is a technique used to evaluate the performance of a model by splitting the dataset into multiple folds and training the model on each fold.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from sklearn.model_selection import train_test_split, KFold\n",
                "from sklearn.datasets import load_iris\n",
                "import numpy as np\n",
                "\n",
                "# Load Iris dataset\n",
                "iris = load_iris()\n",
                "X = iris.data\n",
                "y = iris.target\n",
                "\n",
                "# Split dataset into training and test sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
                "\n",
                "# Convert to tensors\n",
                "X_train = torch.from_numpy(X_train).float()\n",
                "X_test = torch.from_numpy(X_test).float()\n",
                "y_train = torch.from_numpy(y_train).long()\n",
                "y_test = torch.from_numpy(y_test).long()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 79,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([1, 2, 2, 1, 2])\n"
                    ]
                }
            ],
            "source": [
                "# print some y values\n",
                "print(y_train[:5])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Define the Neural Network Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 80,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Net(nn.Module):\n",
                "    def __init__(self, dropout=False):\n",
                "        super(Net, self).__init__()\n",
                "        self.fc1 = nn.Linear(4, 28)\n",
                "        self.dropout = nn.Dropout(0.5) if dropout else nn.Identity()\n",
                "        self.fc2 = nn.Linear(28, 3)\n",
                "\n",
                "    def forward(self, x):\n",
                "        # x = F.relu(self.fc1(x))\n",
                "        # use sigmoid\n",
                "        x = torch.sigmoid(self.fc1(x))\n",
                "        x = self.dropout(x)\n",
                "        x = self.fc2(x)\n",
                "        return x\n",
                "\n",
                "# Initialize two models: one with dropout and one without\n",
                "model_with_dropout = Net(dropout=True)\n",
                "model_without_dropout = Net(dropout=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training and Evaluation Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 81,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train(model, optimizer, criterion, X_train, y_train):\n",
                "    model.train()\n",
                "    optimizer.zero_grad()\n",
                "    outputs = model(X_train)\n",
                "    loss = criterion(outputs, y_train)\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    return loss.item()\n",
                "\n",
                "def evaluate(model, X_val, y_val):\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        outputs = model(X_val)\n",
                "        _, predicted = torch.max(outputs, 1)\n",
                "\n",
                "        correct = (predicted == y_val).sum().item()\n",
                "        accuracy = correct / X_val.size(0)\n",
                "    return accuracy"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Perform 5-Fold Cross Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 82,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting fold 1\n",
                        "Epoch 10, Loss: 1.133414387702942, Accuracy: 0.24444444444444444\n",
                        "Epoch 20, Loss: 1.1098768711090088, Accuracy: 0.24444444444444444\n",
                        "Epoch 30, Loss: 1.0391340255737305, Accuracy: 0.28888888888888886\n",
                        "Epoch 40, Loss: 1.0465450286865234, Accuracy: 0.28888888888888886\n",
                        "Epoch 50, Loss: 1.0011996030807495, Accuracy: 0.7111111111111111\n",
                        "Starting fold 2\n",
                        "Epoch 10, Loss: 0.9753791093826294, Accuracy: 0.7111111111111111\n",
                        "Epoch 20, Loss: 1.005685567855835, Accuracy: 0.7111111111111111\n",
                        "Epoch 30, Loss: 1.030776023864746, Accuracy: 0.7111111111111111\n",
                        "Epoch 40, Loss: 0.9689159393310547, Accuracy: 0.7111111111111111\n",
                        "Epoch 50, Loss: 0.9489995241165161, Accuracy: 0.7111111111111111\n",
                        "Starting fold 3\n",
                        "Epoch 10, Loss: 0.9274333715438843, Accuracy: 0.8222222222222222\n",
                        "Epoch 20, Loss: 0.9316403269767761, Accuracy: 0.7111111111111111\n",
                        "Epoch 30, Loss: 0.8732455968856812, Accuracy: 0.7111111111111111\n",
                        "Epoch 40, Loss: 0.8535342216491699, Accuracy: 0.7111111111111111\n",
                        "Epoch 50, Loss: 0.8499693274497986, Accuracy: 0.7111111111111111\n",
                        "Starting fold 4\n",
                        "Epoch 10, Loss: 0.8302688598632812, Accuracy: 0.8888888888888888\n",
                        "Epoch 20, Loss: 0.7650681734085083, Accuracy: 0.7111111111111111\n",
                        "Epoch 30, Loss: 0.7984701991081238, Accuracy: 0.7111111111111111\n",
                        "Epoch 40, Loss: 0.7559317946434021, Accuracy: 0.7111111111111111\n",
                        "Epoch 50, Loss: 0.724637508392334, Accuracy: 0.7111111111111111\n",
                        "Starting fold 5\n",
                        "Epoch 10, Loss: 0.7358223795890808, Accuracy: 1.0\n",
                        "Epoch 20, Loss: 0.7596132159233093, Accuracy: 0.8\n",
                        "Epoch 30, Loss: 0.7204081416130066, Accuracy: 0.8\n",
                        "Epoch 40, Loss: 0.7297961115837097, Accuracy: 0.8444444444444444\n",
                        "Epoch 50, Loss: 0.7054985761642456, Accuracy: 0.9111111111111111\n",
                        "Starting fold 1\n",
                        "Epoch 10, Loss: 1.0996743440628052, Accuracy: 0.4222222222222222\n",
                        "Epoch 20, Loss: 1.0741455554962158, Accuracy: 0.3111111111111111\n",
                        "Epoch 30, Loss: 1.0540579557418823, Accuracy: 0.28888888888888886\n",
                        "Epoch 40, Loss: 1.0338212251663208, Accuracy: 0.4666666666666667\n",
                        "Epoch 50, Loss: 1.0126844644546509, Accuracy: 0.7111111111111111\n",
                        "Starting fold 2\n",
                        "Epoch 10, Loss: 0.9973822236061096, Accuracy: 0.7111111111111111\n",
                        "Epoch 20, Loss: 0.9763667583465576, Accuracy: 0.7111111111111111\n",
                        "Epoch 30, Loss: 0.9540632963180542, Accuracy: 0.7111111111111111\n",
                        "Epoch 40, Loss: 0.931045413017273, Accuracy: 0.7111111111111111\n",
                        "Epoch 50, Loss: 0.9069873094558716, Accuracy: 0.7111111111111111\n",
                        "Starting fold 3\n",
                        "Epoch 10, Loss: 0.8706516027450562, Accuracy: 0.7111111111111111\n",
                        "Epoch 20, Loss: 0.8455541729927063, Accuracy: 0.7111111111111111\n",
                        "Epoch 30, Loss: 0.8201395869255066, Accuracy: 0.7111111111111111\n",
                        "Epoch 40, Loss: 0.7949490547180176, Accuracy: 0.7111111111111111\n",
                        "Epoch 50, Loss: 0.7699190974235535, Accuracy: 0.7111111111111111\n",
                        "Starting fold 4\n",
                        "Epoch 10, Loss: 0.7510689496994019, Accuracy: 0.7333333333333333\n",
                        "Epoch 20, Loss: 0.7270810008049011, Accuracy: 0.7333333333333333\n",
                        "Epoch 30, Loss: 0.7035172581672668, Accuracy: 0.7555555555555555\n",
                        "Epoch 40, Loss: 0.6808668375015259, Accuracy: 0.7777777777777778\n",
                        "Epoch 50, Loss: 0.6590569615364075, Accuracy: 0.7555555555555555\n",
                        "Starting fold 5\n",
                        "Epoch 10, Loss: 0.6603046655654907, Accuracy: 0.7333333333333333\n",
                        "Epoch 20, Loss: 0.6406060457229614, Accuracy: 0.7777777777777778\n",
                        "Epoch 30, Loss: 0.6216084361076355, Accuracy: 0.8444444444444444\n",
                        "Epoch 40, Loss: 0.6034419536590576, Accuracy: 0.8444444444444444\n",
                        "Epoch 50, Loss: 0.586053729057312, Accuracy: 0.8444444444444444\n"
                    ]
                }
            ],
            "source": [
                "# Set up KFold with 5 splits\n",
                "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
                "\n",
                "# Train and evaluate the models\n",
                "def perform_cross_validation(model, X_train, y_train):\n",
                "    results = []\n",
                "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train)):\n",
                "        print(f'Starting fold {fold+1}')\n",
                "        # Create data subsets for the fold\n",
                "        X_train_fold = X_train[train_idx]\n",
                "        y_train_fold = y_train[train_idx]\n",
                "\n",
                "        # Initialize optimizer and criterion\n",
                "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
                "        criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "        # Training and evaluation for one fold\n",
                "        for epoch in range(50):\n",
                "            loss = train(model, optimizer, criterion, X_train_fold, y_train_fold)\n",
                "            if (epoch+1) % 10 == 0:\n",
                "                accuracy = evaluate(model, X_test, y_test)\n",
                "                print(f'Epoch {epoch+1}, Loss: {loss}, Accuracy: {accuracy}')\n",
                "\n",
                "        results.append({'fold': fold+1, 'final_loss': loss, 'accuracy': accuracy})\n",
                "    return results\n",
                "\n",
                "results_with_dropout = perform_cross_validation(model_with_dropout, X_train, y_train)\n",
                "results_without_dropout = perform_cross_validation(model_without_dropout, X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 83,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "45"
                        ]
                    },
                    "execution_count": 83,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "len(X_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Review Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 84,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Results with Dropout:\n",
                        "Fold 1: Loss: 1.0011996030807495, Accuracy: 0.7111111111111111\n",
                        "Fold 2: Loss: 0.9489995241165161, Accuracy: 0.7111111111111111\n",
                        "Fold 3: Loss: 0.8499693274497986, Accuracy: 0.7111111111111111\n",
                        "Fold 4: Loss: 0.724637508392334, Accuracy: 0.7111111111111111\n",
                        "Fold 5: Loss: 0.7054985761642456, Accuracy: 0.9111111111111111\n",
                        "\n",
                        "Results without Dropout:\n",
                        "Fold 1: Loss: 1.0126844644546509, Accuracy: 0.7111111111111111\n",
                        "Fold 2: Loss: 0.9069873094558716, Accuracy: 0.7111111111111111\n",
                        "Fold 3: Loss: 0.7699190974235535, Accuracy: 0.7111111111111111\n",
                        "Fold 4: Loss: 0.6590569615364075, Accuracy: 0.7555555555555555\n",
                        "Fold 5: Loss: 0.586053729057312, Accuracy: 0.8444444444444444\n"
                    ]
                }
            ],
            "source": [
                "print('Results with Dropout:')\n",
                "for result in results_with_dropout:\n",
                "    print(f\"Fold {result['fold']}: Loss: {result['final_loss']}, Accuracy: {result['accuracy']}\")\n",
                "\n",
                "print('\\nResults without Dropout:')\n",
                "for result in results_without_dropout:\n",
                "    print(f\"Fold {result['fold']}: Loss: {result['final_loss']}, Accuracy: {result['accuracy']}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 85,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Average accuracy with dropout: 0.7511111111111111\n",
                        "Average accuracy without dropout: 0.7466666666666667\n"
                    ]
                }
            ],
            "source": [
                "# calculate average accuracy\n",
                "avg_accuracy_with_dropout = np.mean([result['accuracy'] for result in results_with_dropout])\n",
                "avg_accuracy_without_dropout = np.mean([result['accuracy'] for result in results_without_dropout])\n",
                "print(f'\\nAverage accuracy with dropout: {avg_accuracy_with_dropout}')\n",
                "print(f'Average accuracy without dropout: {avg_accuracy_without_dropout}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "**Understanding Dropout and Overfitting**\n",
                "\n",
                "* Dropout is a technique to fight overfitting and improve neural network generalization\n",
                "* Focus on training performance first, and deal with overfitting once it's clear\n",
                "* Overfitting can manifest in different ways, such as:\n",
                "\t+ Training accuracy increases, but validation accuracy plateaus or decreases\n",
                "\t+ Model performance on training data is good, but poor on new, unseen data\n",
                "\n",
                "**Choosing the Right Dropout Rate**\n",
                "\n",
                "* The default dropout rate of 0.5 may be too severe, especially for convolutional layers\n",
                "* Research suggests that lower dropout rates (0.1, 0.2) may be more effective for convolutional layers\n",
                "* Gradually increasing the dropout rate from the first convolutional layer down the network can be effective\n",
                "* Example architecture:\n",
                "\t+ CONV-1: filter=3x3, size=32, dropout between 0.0-0.1\n",
                "\t+ CONV-2: filter=3x3, size=64, dropout between 0.1-0.25\n",
                "\t+ ...\n",
                "* Cross-validate and optimize hyper-parameters for your specific problem using techniques like random search or Bayesian optimization\n",
                "# its markdown cell so show the image\n",
                "[ ![image](https://i.stack.imgur.com/xvqJI.jpg) ](https://i.stack.imgur.com/xvqJI.jpg)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Hinton dropout paper:\n",
                "https://arxiv.org/pdf/1207.0580.pdf\n",
                "\n",
                "Analysis on the Dropout Effect in Convolutional Neural Networks:\n",
                "http://mipal.snu.ac.kr/images/1/16/Dropout_ACCV2016.pdf\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
